I"ÿ"<h2 id="learning-about-hypotheses-from-data">Learning about hypotheses from data</h2>

<p>The goal of science is to learn about hypotheses or theories through the process of designing experiments and collecting data. 
Hypotheses are unobservable (or, latent): You cannot (in any literal sense) see Newton‚Äôs equations, but they do generate <em>predictions</em> about things you can observe, such as the trajectory of an object through space.</p>

<p>In addition, we are rarely in a position to make <em>deterministic predictions</em> (especially concerning social or psychological phenomena): When we hypothesize ‚ÄúGroup A will exhibit less free will than Group B‚Äù, we are making a statistical (or probabilistic) prediction. We mean: ‚ÄúIn general, Group A will exhibit ‚Ä¶‚Äù
Observing one member of Group B exhibiting more free will than one member of Group A does not falsify the hypothesis.</p>

<p>To learn about hypotheses from data, you should use statistics. 
There are two primary uses of statistics: For learning about aspects of single hypothesis (<em>parameter learning</em>) and for comparing two or more hypotheses against each other (<em>hypothesis testing</em>, or <em>model comparison</em>). 
It is argued that hypothesis testing is logically prior to parameter learning (e.g., you want to know that a thing exists before you go out and study its properties), but the logic and computations associated with hypothesis testing are more complex than parameter learning (reft:Wagenmakers2016:benefits; reft:LW2014). 
Thus, many Bayesian tutorials begin with parameter learning before moving onto hypothesis testing; we will follow suit here.</p>

<h2 id="two-statistics">Two statistics</h2>

<ul>
  <li>Probability is the mathematical foundation of statistics</li>
  <li>What is a probability? (When you say the probability of an event is 0.75, what does that mean?)
    <ul>
      <li>Classical definition (assumes outcomes are equally probable): Number of favorable outcomes divided by the total number of outcomes</li>
      <li>Frequentist / objectivist definition: Limiting relative frequency</li>
      <li>Bayesian / subjectivist definition: Degree of belief</li>
    </ul>
  </li>
</ul>

<h3 id="bayes-theorem-follows-from-the-definition-of-conditional-probability">Bayes Theorem follows from the definition of conditional probability</h3>

<ol>
  <li>
\[P(B \mid A) := \frac{P(B \cap A)}{P(A)}\]
  </li>
  <li>
\[P(A \mid B) := \frac{P(A \cap B)}{P(B)}\]
  </li>
  <li>
\[P(B \cap A) = P(A \cap B)\]
  </li>
  <li>
\[P(B \mid A) \cdot P(A) = P(A \mid B) \cdot P(B)\]
  </li>
  <li>
\[P(B \mid A) = \frac{P(A \mid B) \cdot P(B)}{ P(A) }\]
  </li>
</ol>

<h3 id="bayes-theorem-in-science">Bayes Theorem in science</h3>

\[P(\text{hypothesis} \mid \text{data}) = \frac{P(\text{data} \mid \text{hypothesis}) \times P(\text{hypothesis})}{P(\text{data})}\]

\[= \frac{P(d \mid h) \times P(h)}{\int_{h} P(d \mid h)\times P(h)}\]

<p>In other words:</p>

\[posterior = \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}}\]

<p>Since \(posterior\) has to be a probability, you can often get away with just saying:</p>

\[posterior \propto \text{likelihood} \times \text{prior}\]

<p>and later ‚Äúnormalizing‚Äù the posterior probabilities so they add to 1.</p>

<h4 id="a-note-on-frequentism">A note on frequentism</h4>

<ul>
  <li>Frequentists don‚Äôt believe in \(P(h)\)</li>
  <li>for them: hypotheses are either true or false (doesn‚Äôt make sense to talk about the probability of a hypothesis)</li>
  <li>if you don‚Äôt have priors, you don‚Äôt have posteriors‚Ä¶</li>
  <li>that‚Äôs why p-values are only about \(P(d \mid h)\)</li>
</ul>

<h2 id="why-bayesian">Why Bayesian?</h2>

<ul>
  <li>How we scientists think about statistics refp:Hoekstra2014:misinterpretation</li>
  <li>The inferences we would like to draw refp:Wagenmakers2016:benefits</li>
  <li>Tools are developed for flexibility and continuity with cognitive modeling (described below)</li>
  <li><strong>Why are we not Bayesian already?</strong> Frequentist problems are computationally simpler</li>
</ul>

<h2 id="why-use-a-probabilistic-programming-language-ppl">Why use a probabilistic programming language (PPL)</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Probabilistic_programming_language">probabilistic programming language (PPL)</a> is one specialized for building probabilistic models and <em>performing inference</em> in those models.</p>

<p>Two reasons: (1) An abstraction barrier between the model and the algorithm for inference (a la <code class="language-plaintext highlighter-rouge">lm</code> in R) and (2) Flexibility in model development.</p>

<p>Getting results out of probabilistic models requires <em>performing Bayesian inference</em> on the model. 
Probabilistic programming provides an abstraction barrier (i.e., a separation) between the theoretically-interesting <em>model</em> and the <em>inference algorithm</em> needed to return an answer from the model. 
If the probabilistic language is sufficiently well-developed, one can perform inference on a model even with only cursory knowledge of the algorithm for inference.
We will cover a few (common) algorithms in WebPPL, already part of the ecosystem.
You can use them, just by specifying some options.</p>

<p>Using a full-blown probabilistic <em>language</em> gives you maximal flexibility for model development and is a common language for both data analysis and cognitive modeling. 
The fundamental strokes of model development are (a) basic programming techniques; (b) a primitive way to define random variables (elementary random primitives or ERPs; a feature of probabilistic languages) and (c) a primitive way to perform inference on a model (the <code class="language-plaintext highlighter-rouge">Infer()</code> operator in WebPPL).
By employing these fundamental strokes, one can develop increasingly sophisticated models.</p>

<blockquote>
  <p><strong>Bayesian cognitive models vs. Bayesian data analytic models</strong>: Bayesian cognitive modeling is a research program in Cognitive Science that assumes that human cognition can be modeled as performing Bayesian inference (so-called ‚ÄúBayes in the head‚Äù). Bayesian data analysis is a methodology for use in any empirical science: It assumes a particular interpretation of probability and that scientific questions (hypothesis testing, etc.) can be modeled as Bayesian inference (so-called ‚ÄúBayes in the notebook‚Äù)</p>
</blockquote>

<p>Further, one can write model data analytic models and cognitive models in a PPL.
You can even perform a Bayesian data analysis of a Bayesian cognitive model, a so-called <em>descriptive Bayesian approach</em> refp:tauber2017descriptive.
For example, Bayesian models of cognition require specifying people‚Äôs prior; one can infer what the prior knowledge should be given the observed data and the supposed model of cognition. 
If you are able to specify multiple hypotheses in a probabilistic programming language as well as a space of experiments that could potentially disambiguiate the hypotheses, one can automate the search for the optimal experiment in the same ecosystem refp:ouyangwebppl. 
<!-- Cognitive modeling thought of as a separate domain from data analysis: Everybody does data analysis, not everyone models.  -->
<!-- New perspective: AS you start building data analytic models, this leads into cognitive modeling. (At some point, you transition.) --></p>

<h3 id="other-languages">Other languages</h3>

<p>In this tutorial, we will use the probabilistic programming language <a href="http://webppl.org">WebPPL</a>. There are many other good programming systems for Bayesian modeling. The two most common are:</p>

<ul>
  <li><a href="http://mc-stan.org">STAN</a> (and now, <a href="https://github.com/paul-buerkner/brms"><code class="language-plaintext highlighter-rouge">brms</code></a>)</li>
  <li><a href="http://www.openbugs.net/w/FrontPage">BUGS</a> / JAGS</li>
</ul>

<p>STAN, BUGS, and JAGS are all ‚Äúrestricted languages‚Äù. They are not full probabilistic languages and thus one cannot specify certain models in those languages.</p>

<p>There are many other probabilistic languages. A new language that tries to bring the best of neural network modeling to probabilistic programming is called <a href="http://pyro.ai">Pyro</a></p>

<blockquote>
  <p>Dive into the deep end with a Bayesian data analysis of a Bayesian cognitive model of the ‚ÄúSpinning coins‚Äù problem <a href="https://probmods.org/chapters/14-bayesian-data-analysis.html">here</a>.</p>
</blockquote>

<p>In the <a href="02-introPPL.html">next chapter</a>, we will dig into the basics of probabilistic programming.</p>

<h2 id="references">References</h2>

<p>cite:Hoekstra2014:misinterpretation</p>

<p>cite:LW2014</p>

<p>cite:tauber2017descriptive</p>

<p>cite:ouyangwebppl</p>

<p>cite:Wagenmakers2016:benefits</p>

:ET