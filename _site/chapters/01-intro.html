<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Why analyze data</title>
  <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.6.0/pure-min.css">
  <link rel="stylesheet" href="/bdappl/assets/css/main.css">
  <link rel="stylesheet" href="/bdappl/assets/css/code.css">
  <!-- Katex Script -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;700;900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=PT+Serif:ital@1&display=swap" rel="stylesheet">

  <link rel="canonical" href="http://localhost:4000/bdappl/chapters/01-intro.html">
  <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script>
  <script src="/bdappl/assets/js/parse-bibtex.js"></script>
  <script src="/bdappl/assets/js/main.js"></script>

  <script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.js"></script>
  <link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-editor-1.0.9.css">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>


  <script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.js"></script>
  <link rel="stylesheet" href="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-viz-0.7.11.css">
  <!-- <link rel="stylesheet" href="http://127.0.0.1:8888/home/wp/viz/demo/webppl-viz.css"> -->
  <!-- <script src="http://127.0.0.1:8888/home/wp/viz/demo/webppl-viz.js"></script> -->

  <script src="https://s3-us-west-2.amazonaws.com/cdn.webppl.org/webppl-v0.9.13.js"></script>
</head>

  <body>

    <header class="site-header">
  <a class="site-title">Bayesian <span>Machine Learning</span></a>
</header>


    <div class="page-content-wrapper">
      <div class="page-content">
        <h1 class="chapter-title">Why analyze data</h1>

<h2 id="learning-about-hypotheses-from-data">Learning about hypotheses from data</h2>

<p>The goal of science is to learn about hypotheses or theories through the process of designing experiments and collecting data. 
Hypotheses are unobservable (or, latent): You cannot (in any literal sense) see Newton’s equations, but they do generate <em>predictions</em> about things you can observe, such as the trajectory of an object through space.</p>

<p>In addition, we are rarely in a position to make <em>deterministic predictions</em> (especially concerning social or psychological phenomena): When we hypothesize “Group A will exhibit less free will than Group B”, we are making a statistical (or probabilistic) prediction. We mean: “In general, Group A will exhibit …”
Observing one member of Group B exhibiting more free will than one member of Group A does not falsify the hypothesis.</p>

<p>To learn about hypotheses from data, you should use statistics. 
There are two primary uses of statistics: For learning about aspects of single hypothesis (<em>parameter learning</em>) and for comparing two or more hypotheses against each other (<em>hypothesis testing</em>, or <em>model comparison</em>). 
It is argued that hypothesis testing is logically prior to parameter learning (e.g., you want to know that a thing exists before you go out and study its properties), but the logic and computations associated with hypothesis testing are more complex than parameter learning (reft:Wagenmakers2016:benefits; reft:LW2014). 
Thus, many Bayesian tutorials begin with parameter learning before moving onto hypothesis testing; we will follow suit here.</p>

<h2 id="two-statistics">Two statistics</h2>

<ul>
  <li>Probability is the mathematical foundation of statistics</li>
  <li>What is a probability? (When you say the probability of an event is 0.75, what does that mean?)
    <ul>
      <li>Classical definition (assumes outcomes are equally probable): Number of favorable outcomes divided by the total number of outcomes</li>
      <li>Frequentist / objectivist definition: Limiting relative frequency</li>
      <li>Bayesian / subjectivist definition: Degree of belief</li>
    </ul>
  </li>
</ul>

<h3 id="bayes-theorem-follows-from-the-definition-of-conditional-probability">Bayes Theorem follows from the definition of conditional probability</h3>

<ol>
  <li>
\[P(B \mid A) := \frac{P(B \cap A)}{P(A)}\]
  </li>
  <li>
\[P(A \mid B) := \frac{P(A \cap B)}{P(B)}\]
  </li>
  <li>
\[P(B \cap A) = P(A \cap B)\]
  </li>
  <li>
\[P(B \mid A) \cdot P(A) = P(A \mid B) \cdot P(B)\]
  </li>
  <li>
\[P(B \mid A) = \frac{P(A \mid B) \cdot P(B)}{ P(A) }\]
  </li>
</ol>

<h3 id="bayes-theorem-in-science">Bayes Theorem in science</h3>

\[P(\text{hypothesis} \mid \text{data}) = \frac{P(\text{data} \mid \text{hypothesis}) \times P(\text{hypothesis})}{P(\text{data})}\]

\[= \frac{P(d \mid h) \times P(h)}{\int_{h} P(d \mid h)\times P(h)}\]

<p>In other words:</p>

\[posterior = \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}}\]

<p>Since \(posterior\) has to be a probability, you can often get away with just saying:</p>

\[posterior \propto \text{likelihood} \times \text{prior}\]

<p>and later “normalizing” the posterior probabilities so they add to 1.</p>

<h4 id="a-note-on-frequentism">A note on frequentism</h4>

<ul>
  <li>Frequentists don’t believe in \(P(h)\)</li>
  <li>for them: hypotheses are either true or false (doesn’t make sense to talk about the probability of a hypothesis)</li>
  <li>if you don’t have priors, you don’t have posteriors…</li>
  <li>that’s why p-values are only about \(P(d \mid h)\)</li>
</ul>

<h2 id="why-bayesian">Why Bayesian?</h2>

<ul>
  <li>How we scientists think about statistics refp:Hoekstra2014:misinterpretation</li>
  <li>The inferences we would like to draw refp:Wagenmakers2016:benefits</li>
  <li>Tools are developed for flexibility and continuity with cognitive modeling (described below)</li>
  <li><strong>Why are we not Bayesian already?</strong> Frequentist problems are computationally simpler</li>
</ul>

<h2 id="why-use-a-probabilistic-programming-language-ppl">Why use a probabilistic programming language (PPL)</h2>

<p>A <a href="https://en.wikipedia.org/wiki/Probabilistic_programming_language">probabilistic programming language (PPL)</a> is one specialized for building probabilistic models and <em>performing inference</em> in those models.</p>

<p>Two reasons: (1) An abstraction barrier between the model and the algorithm for inference (a la <code class="language-plaintext highlighter-rouge">lm</code> in R) and (2) Flexibility in model development.</p>

<p>Getting results out of probabilistic models requires <em>performing Bayesian inference</em> on the model. 
Probabilistic programming provides an abstraction barrier (i.e., a separation) between the theoretically-interesting <em>model</em> and the <em>inference algorithm</em> needed to return an answer from the model. 
If the probabilistic language is sufficiently well-developed, one can perform inference on a model even with only cursory knowledge of the algorithm for inference.
We will cover a few (common) algorithms in WebPPL, already part of the ecosystem.
You can use them, just by specifying some options.</p>

<p>Using a full-blown probabilistic <em>language</em> gives you maximal flexibility for model development and is a common language for both data analysis and cognitive modeling. 
The fundamental strokes of model development are (a) basic programming techniques; (b) a primitive way to define random variables (elementary random primitives or ERPs; a feature of probabilistic languages) and (c) a primitive way to perform inference on a model (the <code class="language-plaintext highlighter-rouge">Infer()</code> operator in WebPPL).
By employing these fundamental strokes, one can develop increasingly sophisticated models.</p>

<blockquote>
  <p><strong>Bayesian cognitive models vs. Bayesian data analytic models</strong>: Bayesian cognitive modeling is a research program in Cognitive Science that assumes that human cognition can be modeled as performing Bayesian inference (so-called “Bayes in the head”). Bayesian data analysis is a methodology for use in any empirical science: It assumes a particular interpretation of probability and that scientific questions (hypothesis testing, etc.) can be modeled as Bayesian inference (so-called “Bayes in the notebook”)</p>
</blockquote>

<p>Further, one can write model data analytic models and cognitive models in a PPL.
You can even perform a Bayesian data analysis of a Bayesian cognitive model, a so-called <em>descriptive Bayesian approach</em> refp:tauber2017descriptive.
For example, Bayesian models of cognition require specifying people’s prior; one can infer what the prior knowledge should be given the observed data and the supposed model of cognition. 
If you are able to specify multiple hypotheses in a probabilistic programming language as well as a space of experiments that could potentially disambiguiate the hypotheses, one can automate the search for the optimal experiment in the same ecosystem refp:ouyangwebppl. 
<!-- Cognitive modeling thought of as a separate domain from data analysis: Everybody does data analysis, not everyone models.  -->
<!-- New perspective: AS you start building data analytic models, this leads into cognitive modeling. (At some point, you transition.) --></p>

<h3 id="other-languages">Other languages</h3>

<p>In this tutorial, we will use the probabilistic programming language <a href="http://webppl.org">WebPPL</a>. There are many other good programming systems for Bayesian modeling. The two most common are:</p>

<ul>
  <li><a href="http://mc-stan.org">STAN</a> (and now, <a href="https://github.com/paul-buerkner/brms"><code class="language-plaintext highlighter-rouge">brms</code></a>)</li>
  <li><a href="http://www.openbugs.net/w/FrontPage">BUGS</a> / JAGS</li>
</ul>

<p>STAN, BUGS, and JAGS are all “restricted languages”. They are not full probabilistic languages and thus one cannot specify certain models in those languages.</p>

<p>There are many other probabilistic languages. A new language that tries to bring the best of neural network modeling to probabilistic programming is called <a href="http://pyro.ai">Pyro</a></p>

<blockquote>
  <p>Dive into the deep end with a Bayesian data analysis of a Bayesian cognitive model of the “Spinning coins” problem <a href="https://probmods.org/chapters/14-bayesian-data-analysis.html">here</a>.</p>
</blockquote>

<p>In the <a href="02-introPPL.html">next chapter</a>, we will dig into the basics of probabilistic programming.</p>

<h2 id="references">References</h2>

<p>cite:Hoekstra2014:misinterpretation</p>

<p>cite:LW2014</p>

<p>cite:tauber2017descriptive</p>

<p>cite:ouyangwebppl</p>

<p>cite:Wagenmakers2016:benefits</p>



<hr>

<a href="/">Table of Contents</a>

      </div>
    </div>

    <footer class="site-footer">
  <a href="https://github.com/JoaoNM/Bayesian-ML" id="github-edit-link">Edit on Github</a>
</footer>

<script type="text/javascript">
  $("#github-edit-link").attr("href", github_page_url("/chapters/01-intro.html"))
</script>


  </body>

</html>
